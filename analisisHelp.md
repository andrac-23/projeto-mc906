# https://github.com/ultralytics/ultralytics/issues/7307 

# F1 Confidence: Shows the F1 score (harmonic mean of precision and recall) at different confidence thresholds. A higher peak suggests better model performance.

# Precision-Recall Curve: Illustrates the trade-off between precision and recall for different thresholds. A model that reaches closer to the top-right corner is better.

# Precision-Confidence Curve: Displays how precision changes with different confidence levels. Ideally, you want high precision across all confidence levels.

# Recall-Confidence Curve: Shows the recall at various confidence thresholds. You're looking for high recall across the board.

# Confusion Matrix: Summarizes the performance of classification. Diagonal values represent correct predictions, while off-diagonal values indicate misclassifications.